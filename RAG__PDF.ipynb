{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9299042,
          "sourceType": "datasetVersion",
          "datasetId": 5630231
        },
        {
          "sourceId": 105881,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 88718,
          "modelId": 112929
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "RAG__PDF",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'energy:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5630231%2F9299042%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240902%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240902T122405Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D832e952f80b12640300e484d169b7c53e5aaae05ee973ea542835162a823bdf58b07307c53bd63baa7265c3d1044f942e4daaf28de64b85e88d686f4fe7f040536711ce305f500ef4a20dbc77325f145abac35909f98b0d49964cecab25c0287af48e96e437547e1eccc75271f1cf49f296acfab4b18a9248aae08bd904be2c007027ae7fc366c2336a6e579bf2260b90fddced66cdab994b8122acdd344bdbf89ab7f73e722738ff130ffa3dbd086641a5dfe675eb88a5820c135f506d35457f19594d4ed93aed3ee57f0f1ce4d610e399ffba788c7de448ed2f1663074feffac7316a3627e447ba5bf4a55befea31c6e0ec9bba33cf80ebed97b2835fcd203,llama2/pytorch/default/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F88718%2F105881%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240902%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240902T122405Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dce57ccfe6f893e7dc9008ecb702ba9c2f215fc48515a0dc22597a53c81d22fb16d500746f3b988e461d75c70991c9fa6b65900dbdbf747b260d188d9c5f7cd77acbdec9f2ab896213123bc3ccf269cb2a4848d3a346dc80bca9e3ec9d927d1c91891c8a96b780acaa685e0a7eec688b11d351f22dcbdae973bf566d7ca0150f14f59112a2843f7617288b81c703098ee8c59133c2db9d2469444a9d302e4e69f2364cea131101f5baf8b2810435cb6ba65e051e4ece2da1928f28ca0e7be523bfaf32ddb23631fbb379fb28f5e1a91347734a8ab0a383863e06a8b83124c4668d72b1bc482d8eab2afee3a49c41e10a990d9f49a52e38207da079189d4e80230'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "jos2smwKsU3e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-02T11:54:45.958008Z",
          "iopub.execute_input": "2024-09-02T11:54:45.958378Z",
          "iopub.status.idle": "2024-09-02T11:54:45.967914Z",
          "shell.execute_reply.started": "2024-09-02T11:54:45.958345Z",
          "shell.execute_reply": "2024-09-02T11:54:45.966936Z"
        },
        "trusted": true,
        "id": "8dcuCe5usU3h",
        "outputId": "1a0774c5-81b4-4787-f182-41202058d906"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/energy/1Ch2.pdf\n/kaggle/input/llama2/pytorch/default/1/llama-2-7b-chat.Q4_K_M.gguf\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I’ve been working in the field of **conversational AI** lately. My latest project has been to create a custom chatbot that can answer questions based on specific PDF documents. I’m using the **Retrieval Augmented Generation (RAG)** to make this happen."
      ],
      "metadata": {
        "id": "bfDtwU-psU3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 langchain langchain_community sentence_transformers llama-cpp-python faiss-gpu tiktoken torch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:54:46.28711Z",
          "iopub.execute_input": "2024-09-02T11:54:46.287495Z",
          "iopub.status.idle": "2024-09-02T11:56:52.48259Z",
          "shell.execute_reply.started": "2024-09-02T11:54:46.287456Z",
          "shell.execute_reply": "2024-09-02T11:56:52.481546Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "k7l__HJMsU3i",
        "outputId": "8a59a823-ba05-479e-bae3-34e03a67b8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting PyPDF2\n  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nCollecting langchain\n  Using cached langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain_community\n  Using cached langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\nCollecting sentence_transformers\n  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nCollecting llama-cpp-python\n  Using cached llama_cpp_python-0.2.90.tar.gz (63.8 MB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting tiktoken\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.35 (from langchain)\n  Downloading langchain_core-0.2.37-py3-none-any.whl.metadata (6.2 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.108-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.0)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.24.6)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (21.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (1.33)\nCollecting packaging>=20.9 (from huggingface-hub>=0.15.1->sentence_transformers)\n  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain) (2.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.2.15-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.2.15-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.37-py3-none-any.whl (396 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.108-py3-none-any.whl (150 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing cached packaging-24.1-py3-none-any.whl (53 kB)\nBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl size=3415110 sha256=5168619342e4821151093109c75cafac660632660bf726a9bd83309a6f775d42\n  Stored in directory: /root/.cache/pip/wheels/3d/67/02/f950031435db4a5a02e6269f6adb6703bf1631c3616380f3c6\nSuccessfully built llama-cpp-python\nInstalling collected packages: faiss-gpu, PyPDF2, packaging, diskcache, tiktoken, llama-cpp-python, langsmith, langchain-core, sentence_transformers, langchain-text-splitters, langchain, langchain_community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.2 requires cubinlinker, which is not installed.\ncudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.4 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.4 requires shapely<2.1,>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyPDF2-3.0.1 diskcache-5.6.3 faiss-gpu-1.7.2 langchain-0.2.15 langchain-core-0.2.37 langchain-text-splitters-0.2.2 langchain_community-0.2.15 langsmith-0.1.108 llama-cpp-python-0.2.90 packaging-24.1 sentence_transformers-3.0.1 tiktoken-0.7.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the required libraries\n",
        "import PyPDF2\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings # import hf embedding\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:56:52.485033Z",
          "iopub.execute_input": "2024-09-02T11:56:52.485458Z",
          "iopub.status.idle": "2024-09-02T11:57:15.548458Z",
          "shell.execute_reply.started": "2024-09-02T11:56:52.485411Z",
          "shell.execute_reply": "2024-09-02T11:57:15.547586Z"
        },
        "trusted": true,
        "id": "MNjWc78wsU3j",
        "outputId": "de4cadf7-97d2-4618-df21-12fce4a8d6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_docs=[\"/kaggle/input/energy/1Ch2.pdf\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:15.549593Z",
          "iopub.execute_input": "2024-09-02T11:57:15.550242Z",
          "iopub.status.idle": "2024-09-02T11:57:15.554596Z",
          "shell.execute_reply.started": "2024-09-02T11:57:15.550205Z",
          "shell.execute_reply": "2024-09-02T11:57:15.553566Z"
        },
        "trusted": true,
        "id": "_clkJ-2PsU3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_docs(pdf_docs):\n",
        "    docs = []\n",
        "    metadata = []\n",
        "    content = []\n",
        "\n",
        "    for pdf in pdf_docs:\n",
        "\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "        for index, text in enumerate(pdf_reader.pages):\n",
        "            doc_page = {'title': pdf + \" page \" + str(index + 1),\n",
        "                        'content': pdf_reader.pages[index].extract_text()}\n",
        "            docs.append(doc_page)\n",
        "    for doc in docs:\n",
        "        content.append(doc[\"content\"])\n",
        "        metadata.append({\n",
        "            \"title\": doc[\"title\"]\n",
        "        })\n",
        "    print(\"Content and metadata are extracted from the documents\")\n",
        "    return content, metadata"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:15.55718Z",
          "iopub.execute_input": "2024-09-02T11:57:15.557994Z",
          "iopub.status.idle": "2024-09-02T11:57:15.638299Z",
          "shell.execute_reply.started": "2024-09-02T11:57:15.557958Z",
          "shell.execute_reply": "2024-09-02T11:57:15.637335Z"
        },
        "trusted": true,
        "id": "ftaxeOJQsU3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:15.639586Z",
          "iopub.execute_input": "2024-09-02T11:57:15.640057Z",
          "iopub.status.idle": "2024-09-02T11:57:15.688015Z",
          "shell.execute_reply.started": "2024-09-02T11:57:15.640014Z",
          "shell.execute_reply": "2024-09-02T11:57:15.687034Z"
        },
        "trusted": true,
        "id": "hdLJN5d4sU3j",
        "outputId": "5e33bbda-5631-4d70-f51d-f219f475c4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_chunks(content, metadata):\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=512,\n",
        "        chunk_overlap=256,\n",
        "    )\n",
        "    split_docs = text_splitter.create_documents(content, metadatas=metadata)\n",
        "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
        "    return split_docs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:15.68903Z",
          "iopub.execute_input": "2024-09-02T11:57:15.689328Z",
          "iopub.status.idle": "2024-09-02T11:57:15.696898Z",
          "shell.execute_reply.started": "2024-09-02T11:57:15.689297Z",
          "shell.execute_reply": "2024-09-02T11:57:15.696025Z"
        },
        "trusted": true,
        "id": "jaeh3uFJsU3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_into_vectordb(split_docs):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': device})\n",
        "    db = FAISS.from_documents(split_docs, embeddings)\n",
        "\n",
        "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
        "    db.save_local(DB_FAISS_PATH)\n",
        "    return db"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:15.697893Z",
          "iopub.execute_input": "2024-09-02T11:57:15.698253Z",
          "iopub.status.idle": "2024-09-02T11:57:15.707253Z",
          "shell.execute_reply.started": "2024-09-02T11:57:15.69822Z",
          "shell.execute_reply": "2024-09-02T11:57:15.706376Z"
        },
        "trusted": true,
        "id": "wXdMZJeisU3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"[INST]\n",
        "As an AI, provide accurate and relevant information based on the provided document. Your responses should adhere to the following guidelines:\n",
        "- Answer the question based on the provided documents.\n",
        "- Be direct and factual, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
        "- Maintain an ethical and unbiased tone, avoiding harmful or offensive content.\n",
        "- If the document does not contain relevant information, state \"I cannot provide an answer based on the provided document.\"\n",
        "- Avoid using confirmatory phrases like \"Yes, you are correct\" or any similar validation in your responses.\n",
        "- Do not fabricate information or include questions in your responses.\n",
        "- do not prompt to select answers. do not ask me questions\n",
        "{question}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "def get_conversation_chain(vectordb):\n",
        "    llama_llm = LlamaCpp(\n",
        "        model_path=\"/kaggle/input/llama2/pytorch/default/1/llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "        temperature=0.75,\n",
        "        max_tokens=300,\n",
        "        top_p=1,\n",
        "        callback_manager=callback_manager,\n",
        "        n_ctx=3000,\n",
        "        device=device  # Ensures LlamaCpp uses the GPU\n",
        "    )\n",
        "\n",
        "    retriever = vectordb.as_retriever()\n",
        "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True, output_key='answer')\n",
        "\n",
        "    conversation_chain = (ConversationalRetrievalChain.from_llm\n",
        "                          (llm=llama_llm,\n",
        "                           retriever=retriever,\n",
        "                           #condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
        "                           memory=memory,\n",
        "                           return_source_documents=True))\n",
        "    print(\"Conversational Chain created for the LLM using the vector store\")\n",
        "    return conversation_chain\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:15.708772Z",
          "iopub.execute_input": "2024-09-02T11:57:15.709165Z",
          "iopub.status.idle": "2024-09-02T11:57:15.722411Z",
          "shell.execute_reply.started": "2024-09-02T11:57:15.709122Z",
          "shell.execute_reply": "2024-09-02T11:57:15.721613Z"
        },
        "trusted": true,
        "id": "_AiwJjc-sU3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_answer_against_sources(response_answer, source_documents):\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    similarity_threshold = 0.5\n",
        "    source_texts = [doc.page_content for doc in source_documents]\n",
        "\n",
        "    answer_embedding = model.encode(response_answer, convert_to_tensor=True).to(device)\n",
        "    source_embeddings = model.encode(source_texts, convert_to_tensor=True).to(device)\n",
        "\n",
        "    cosine_scores = util.pytorch_cos_sim(answer_embedding, source_embeddings)\n",
        "\n",
        "\n",
        "    if any(score.item() > similarity_threshold for score in cosine_scores[0]):\n",
        "        return True\n",
        "\n",
        "    return False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:15.723444Z",
          "iopub.execute_input": "2024-09-02T11:57:15.723725Z",
          "iopub.status.idle": "2024-09-02T11:57:15.736644Z",
          "shell.execute_reply.started": "2024-09-02T11:57:15.723694Z",
          "shell.execute_reply": "2024-09-02T11:57:15.735877Z"
        },
        "trusted": true,
        "id": "iMXIp16ssU3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content, metadata = prepare_docs(pdf_docs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:15.739137Z",
          "iopub.execute_input": "2024-09-02T11:57:15.739452Z",
          "iopub.status.idle": "2024-09-02T11:57:16.343657Z",
          "shell.execute_reply.started": "2024-09-02T11:57:15.739413Z",
          "shell.execute_reply": "2024-09-02T11:57:16.342692Z"
        },
        "trusted": true,
        "id": "3WbJNCfxsU3k",
        "outputId": "4bcbc6f3-33e3-48bb-cb46-cb93ae02b1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Content and metadata are extracted from the documents\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_docs = get_text_chunks(content, metadata)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:16.344972Z",
          "iopub.execute_input": "2024-09-02T11:57:16.34573Z",
          "iopub.status.idle": "2024-09-02T11:57:17.807817Z",
          "shell.execute_reply.started": "2024-09-02T11:57:16.345684Z",
          "shell.execute_reply": "2024-09-02T11:57:17.806869Z"
        },
        "trusted": true,
        "id": "jtlZzryWsU3k",
        "outputId": "e9b5bfb0-c5a3-411b-cf65-ce1a10d4a62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Documents are split into 25 passages\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb=ingest_into_vectordb(split_docs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:17.809208Z",
          "iopub.execute_input": "2024-09-02T11:57:17.809614Z",
          "iopub.status.idle": "2024-09-02T11:57:21.291164Z",
          "shell.execute_reply.started": "2024-09-02T11:57:17.809569Z",
          "shell.execute_reply": "2024-09-02T11:57:21.290118Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "c03d5e0f063343c187d1556dda1fa47e",
            "cf53f1a358c5450f90ff25bea0bf887c",
            "dd30ead06f844abe9509d906d705eb34",
            "75ebe47aa82344eaad2a335f3efc5059",
            "830e142020b74f7d99e92ede03c99f40",
            "22e73e6aefe748689fb1e88163d83292",
            "58d2a4ab41c544008156674757a21b2a",
            "b8ecb87ba5f94d30afd448a10c74e416",
            "c2b3295b0d2d4958a344ab58b58833dd",
            "d95705df5fcf4804a9ed33fa10103be6",
            "8dbf80a8a9c9465b90120231d2690031"
          ]
        },
        "id": "4BdiHhzSsU3l",
        "outputId": "75af0ccf-7c09-4995-c065-a88a1d40a47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/1138239711.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': device})\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c03d5e0f063343c187d1556dda1fa47e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf53f1a358c5450f90ff25bea0bf887c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd30ead06f844abe9509d906d705eb34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75ebe47aa82344eaad2a335f3efc5059"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "830e142020b74f7d99e92ede03c99f40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22e73e6aefe748689fb1e88163d83292"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58d2a4ab41c544008156674757a21b2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8ecb87ba5f94d30afd448a10c74e416"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2b3295b0d2d4958a344ab58b58833dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d95705df5fcf4804a9ed33fa10103be6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dbf80a8a9c9465b90120231d2690031"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_chain=get_conversation_chain(vectordb)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:21.292456Z",
          "iopub.execute_input": "2024-09-02T11:57:21.292737Z",
          "iopub.status.idle": "2024-09-02T11:57:50.044123Z",
          "shell.execute_reply.started": "2024-09-02T11:57:21.292706Z",
          "shell.execute_reply": "2024-09-02T11:57:50.043193Z"
        },
        "trusted": true,
        "id": "lXTIlM-9sU3l",
        "outputId": "9a8af584-76b5-4b52-d99b-106afceda103"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/1610560582.py:17: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n  llama_llm = LlamaCpp(\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /kaggle/input/llama2/pytorch/default/1/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \nllm_load_print_meta: general.name     = LLaMA v2\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.14 MiB\nllm_load_tensors:        CPU buffer size =  3891.24 MiB\n..................................................................................................\nllama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\nllama_new_context_with_model: n_ctx      = 3008\nllama_new_context_with_model: n_batch    = 32\nllama_new_context_with_model: n_ubatch   = 32\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =  1504.00 MiB\nllama_new_context_with_model: KV self size  = 1504.00 MiB, K (f16):  752.00 MiB, V (f16):  752.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:        CPU compute buffer size =    14.12 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Conversational Chain created for the LLM using the vector store\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: llama-2\n/tmp/ipykernel_36/446300897.py:1: UserWarning: WARNING! device is not default parameter.\n                device was transferred to model_kwargs.\n                Please confirm that device is what you intended.\n  conversation_chain=get_conversation_chain(vectordb)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LETS DO INFERENCE NOW\n",
        "user_question = \"Define various types of energy?\"\n",
        "response=conversation_chain({\"question\": user_question})\n",
        "print(\"A: \",response['answer'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-02T11:57:50.045509Z",
          "iopub.execute_input": "2024-09-02T11:57:50.046404Z",
          "iopub.status.idle": "2024-09-02T12:06:23.315079Z",
          "shell.execute_reply.started": "2024-09-02T11:57:50.046367Z",
          "shell.execute_reply": "2024-09-02T12:06:23.314082Z"
        },
        "trusted": true,
        "id": "X7ymeSoKsU3l",
        "outputId": "dff66179-584c-42c0-9035-a8d0eaac1bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/1651069777.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n  response=conversation_chain({\"question\": user_question})\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " The question is seeking to determine the different forms of energy, and a comprehensive answer would be:\n\nThere are several types of energy, including:\n\n1. Gravitational Energy - the energy of place or position, such as water in a reservoir behind a hydroelectric dam.\n2. Kinetic Energy - the energy of motion, including waves, electrons, atoms, and molecules. Examples include radiant energy, thermal energy, and sound. \n3. Radiant Energy - electromagnetic energy that travels in transverse waves, including visible light, x-rays, gamma rays, and radio waves. An example of radiant energy is solar energy.\n4. Thermal Energy - the internal energy of substances, resulting from the vibration and movement of atoms and molecules within the substance. Examples include geothermal energy and heat.\n5. Motion - the movement of objects or substances from one place to another, including wind and hydropower.\n6. Sound Energy - the movement of energy through substances in longitudinal waves, such as sound waves. \nBy understanding these various forms of energy, we can better appreciate their interconnectedness and how they contribute to our daily lives.",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\nllama_print_timings:        load time =    1416.68 ms\nllama_print_timings:      sample time =     138.29 ms /   268 runs   (    0.52 ms per token,  1938.01 tokens per second)\nllama_print_timings: prompt eval time =  405682.76 ms /  2096 tokens (  193.55 ms per token,     5.17 tokens per second)\nllama_print_timings:        eval time =  106543.94 ms /   268 runs   (  397.55 ms per token,     2.52 tokens per second)\nllama_print_timings:       total time =  513141.79 ms /  2364 tokens\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "A:   The question is seeking to determine the different forms of energy, and a comprehensive answer would be:\n\nThere are several types of energy, including:\n\n1. Gravitational Energy - the energy of place or position, such as water in a reservoir behind a hydroelectric dam.\n2. Kinetic Energy - the energy of motion, including waves, electrons, atoms, and molecules. Examples include radiant energy, thermal energy, and sound. \n3. Radiant Energy - electromagnetic energy that travels in transverse waves, including visible light, x-rays, gamma rays, and radio waves. An example of radiant energy is solar energy.\n4. Thermal Energy - the internal energy of substances, resulting from the vibration and movement of atoms and molecules within the substance. Examples include geothermal energy and heat.\n5. Motion - the movement of objects or substances from one place to another, including wind and hydropower.\n6. Sound Energy - the movement of energy through substances in longitudinal waves, such as sound waves. \nBy understanding these various forms of energy, we can better appreciate their interconnectedness and how they contribute to our daily lives.\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}